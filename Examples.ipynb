{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DB connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hostname' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-ffa36f1cde56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Создаем соединение с нашей базой данных\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mconn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpsycopg2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhostname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0musername\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpassword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdbname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Использование with в psycopg2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hostname' is not defined"
     ]
    }
   ],
   "source": [
    "# Импортируем библиотеку, соответствующую типу нашей базы данных \n",
    "import psycopg2\n",
    "\n",
    "# Создаем соединение с нашей базой данных\n",
    "\n",
    "conn = psycopg2.connect(host=hostname, user=username, password=password, dbname=database)\n",
    "\n",
    "# Использование with в psycopg2\n",
    "\n",
    "# with psycopg2.connect(host=hostname, user=username, password=password, dbname=database) as conn:\n",
    "#     with conn.cursor() as cur:\n",
    "\n",
    "# Создаем курсор - это специальный объект который делает запросы и получает их результаты\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Делаем SELECT запрос к базе данных, используя обычный SQL-синтаксис\n",
    "cursor.execute(\"SELECT Name FROM Artist ORDER BY Name LIMIT 3\")\n",
    "\n",
    "# Получаем результат сделанного запроса\n",
    "results = cursor.fetchall()\n",
    "results2 =  cursor.fetchall()\n",
    "\n",
    "print(results)   # [('A Cor Do Som',), ('Aaron Copland & London Symphony Orchestra',), ('Aaron Goldberg',)]\n",
    "print(results2)  # []\n",
    "\n",
    "# Делаем INSERT запрос к базе данных, используя обычный SQL-синтаксис\n",
    "cursor.execute(\"insert into Artist values (Null, 'A Aagrh!') \")\n",
    "\n",
    "# Если мы не просто читаем, но и вносим изменения в базу данных - необходимо сохранить транзакцию\n",
    "conn.commit()\n",
    "\n",
    "# Проверяем результат\n",
    "cursor.execute(\"SELECT Name FROM Artist ORDER BY Name LIMIT 3\")\n",
    "results = cursor.fetchall()\n",
    "print(results)  # [('A Aagrh!',), ('A Cor Do Som',), ('Aaron Copland & London Symphony Orchestra',)]\n",
    "\n",
    "# Разбиваем запрос на несколько строк в тройных кавычках\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "  SELECT name\n",
    "  FROM Artist\n",
    "  ORDER BY Name LIMIT 3\n",
    "\"\"\")\n",
    "\n",
    "# Объединяем запросы к базе данных в один вызов метода\n",
    "\n",
    "cursor.executescript(\"\"\"\n",
    " insert into Artist values (Null, 'A Aagrh!');\n",
    " insert into Artist values (Null, 'A Aagrh-2!');\n",
    "\"\"\")\n",
    "\n",
    "# C подставновкой по порядку на места знаков вопросов:\n",
    "cursor.execute(\"SELECT Name FROM Artist ORDER BY Name LIMIT ?\", ('2'))\n",
    "\n",
    "# И с использованием именнованных замен:\n",
    "cursor.execute(\"SELECT Name from Artist ORDER BY Name LIMIT :limit\", {\"limit\": 3})\n",
    "\n",
    "# Делаем множественную вставку строк проходя по коллекции с помощью метода курсора .executemany()\n",
    "\n",
    "# Обратите внимание, даже передавая одно значение - его нужно передавать кортежем!\n",
    "# Именно по этому тут используется запятая в скобках!\n",
    "new_artists = [\n",
    "    ('A Aagrh!',),\n",
    "    ('A Aagrh!-2',),\n",
    "    ('A Aagrh!-3',),\n",
    "]\n",
    "cursor.executemany(\"insert into Artist values (Null, ?);\", new_artists)\n",
    "\n",
    "# Получаем результаты по одному, используя метод курсора .fetchone()\n",
    "\n",
    "# Он всегда возвращает кортеж или None. если запрос пустой.\n",
    "\n",
    "cursor.execute(\"SELECT Name FROM Artist ORDER BY Name LIMIT 3\")\n",
    "print(cursor.fetchone())    # ('A Cor Do Som',)\n",
    "print(cursor.fetchone())    # ('Aaron Copland & London Symphony Orchestra',)\n",
    "print(cursor.fetchone())    # ('Aaron Goldberg',)\n",
    "print(cursor.fetchone())    # None\n",
    "\n",
    "# Использование курсора как итератора\n",
    "for row in cursor.execute('SELECT Name from Artist ORDER BY Name LIMIT 3'):\n",
    "        print(row)\n",
    "\n",
    "#Повышаем устойчивость кода        \n",
    "        \n",
    "try:\n",
    "    cursor.execute(sql_statement)\n",
    "    result = cursor.fetchall()\n",
    "except sqlite3.DatabaseError as err:       \n",
    "    print(\"Error: \", err)\n",
    "else:\n",
    "    conn.commit()\n",
    "    \n",
    "# Ипользование row_factory    \n",
    "    \n",
    "def dict_factory(cursor, row):\n",
    "    d = {}\n",
    "    for idx, col in enumerate(cursor.description):\n",
    "        d[col[0]] = row[idx]\n",
    "    return d\n",
    "\n",
    "con = sqlite3.connect(\":memory:\")\n",
    "con.row_factory = dict_factory\n",
    "cur = con.cursor()\n",
    "cur.execute(\"select 1 as a\")\n",
    "print(cur.fetchone()[\"a\"])    \n",
    "\n",
    "# Не забываем закрыть соединение с базой данных\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание коннекта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_connection(db_name, db_user, db_password, db_host, db_port):\n",
    "    connection = None\n",
    "    try:\n",
    "        connection = psycopg2.connect(\n",
    "            database=db_name,\n",
    "            user=db_user,\n",
    "            password=db_password,\n",
    "            host=db_host,\n",
    "            port=db_port,\n",
    "        )\n",
    "        print(\"Connection to PostgreSQL DB successful\")\n",
    "    except OperationalError as e:\n",
    "        print(f\"The error '{e}' occurred\")\n",
    "    return connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = create_connection(\"postgres\", \"postgres\", \"abc123\", \"127.0.0.1\", \"5432\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выполнение запроса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(connection, query):\n",
    "    connection.autocommit = True\n",
    "    cursor = connection.cursor()\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        print(\"Query executed successfully\")\n",
    "    except OperationalError as e:\n",
    "        print(f\"The error '{e}' occurred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_users_table = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS users (\n",
    "  id SERIAL PRIMARY KEY,\n",
    "  name TEXT NOT NULL, \n",
    "  age INTEGER,\n",
    "  gender TEXT,\n",
    "  nationality TEXT\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "execute_query(connection, create_users_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание таблицы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table():\n",
    "    \"\"\"\n",
    "    Creates a \"users\" table in the python_app database. \n",
    "    \"\"\"\n",
    "    connection = False\n",
    "\n",
    "    try:\n",
    "        ##### Establishes a connection with the python_app database and creates a cursor object that will be used to execute SQL commands #####\n",
    "        connection = psycopg2.connect(host = \"127.0.0.1\", database = \"python_app\", port = \"5432\", user = \"postgres\", password = \"randompassword\")\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        ##### Create a database table ###\n",
    "        create_table_query = '''CREATE TABLE users\n",
    "                (id BIGSERIAL NOT NULL PRIMARY KEY,\n",
    "                username VARCHAR(50) NOT NULL,\n",
    "                email TEXT NOT NULL UNIQUE,\n",
    "                password_hash TEXT NOT NULL); '''\n",
    "        \n",
    "        cursor.execute(create_table_query)\n",
    "        connection.commit()\n",
    "        print(\"Users table added to python_app database.\")\n",
    "\n",
    "    except (Exception, Error) as error:\n",
    "        print(\"An error occured while trying to connect to the python_app database\")\n",
    "\n",
    "    finally:\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            print(\"Connection to python_app database has now been closed\")\n",
    "\n",
    "##### Main Code #####\n",
    "create_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вставка записей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = [\n",
    "    (\"Happy\", \"I am feeling very happy today\", 1),\n",
    "    (\"Hot Weather\", \"The weather is very hot today\", 2),\n",
    "    (\"Help\", \"I need some help with my work\", 2),\n",
    "    (\"Great News\", \"I am getting married\", 1),\n",
    "    (\"Interesting Game\", \"It was a fantastic game of tennis\", 5),\n",
    "    (\"Party\", \"Anyone up for a late-night party today?\", 3),\n",
    "]\n",
    "\n",
    "post_records = \", \".join([\"%s\"] * len(posts))\n",
    "\n",
    "insert_query = (\n",
    "    f\"INSERT INTO posts (title, description, user_id) VALUES {post_records}\"\n",
    ")\n",
    "\n",
    "connection.autocommit = True\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(insert_query, posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выбор данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_read_query(connection, query):\n",
    "    cursor = connection.cursor()\n",
    "    result = None\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        result = cursor.fetchall()\n",
    "        return result\n",
    "    except OperationalError as e:\n",
    "        print(f\"The error '{e}' occurred\")\n",
    "\n",
    "select_users = \"SELECT * FROM users\"\n",
    "users = execute_read_query(connection, select_users)\n",
    "\n",
    "for user in users:\n",
    "    print(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обновление данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_post_description = \"\"\"\n",
    "UPDATE\n",
    "  posts\n",
    "SET\n",
    "  description = \"The weather has become pleasant now\"\n",
    "WHERE\n",
    "  id = 2\n",
    "\"\"\"\n",
    "\n",
    "execute_query(connection, update_post_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Удаление данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_comment = \"DELETE FROM comments WHERE id = 5\"\n",
    "execute_query(connection, delete_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Работа с датами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import datetime\n",
    "from psycopg2 import Error\n",
    "\n",
    "try:\n",
    "    # Подключиться к существующей базе данных\n",
    "    connection = psycopg2.connect(user=\"postgres\",\n",
    "                                  # пароль, который указали при установке PostgreSQL\n",
    "                                  password=\"1111\",\n",
    "                                  host=\"127.0.0.1\",\n",
    "                                  port=\"5432\",\n",
    "                                  database=\"postgres_db\")\n",
    "\n",
    "    cursor = connection.cursor()\n",
    "    # Выполнение SQL-запроса для вставки даты и времени в таблицу\n",
    "    insert_query = \"\"\" INSERT INTO item (item_Id, item_name, purchase_time, price)\n",
    "                              VALUES (%s, %s, %s, %s)\"\"\"\n",
    "    item_purchase_time = datetime.datetime.now()\n",
    "    item_tuple = (12, \"Keyboard\", item_purchase_time, 150)\n",
    "    cursor.execute(insert_query, item_tuple)\n",
    "    connection.commit()\n",
    "    print(\"1 элемент успешно добавлен\")\n",
    "\n",
    "    # Считать значение времени покупки PostgreSQL в Python datetime\n",
    "    cursor.execute(\"SELECT purchase_time from item where item_id = 12\")\n",
    "    purchase_datetime = cursor.fetchone()\n",
    "    print(\"Дата покупки товара\", purchase_datetime[0].date())\n",
    "    print(\"Время покупки товара\", purchase_datetime[0].time())\n",
    "\n",
    "except (Exception, Error) as error:\n",
    "    print(\"Ошибка при работе с PostgreSQL\", error)\n",
    "finally:\n",
    "    if connection:\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        print(\"Соединение с PostgreSQL закрыто\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функции PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE FUNCTION reffunc(refcursor) RETURNS refcursor AS $$\n",
    "BEGIN\n",
    "    OPEN $1 FOR SELECT col FROM test;\n",
    "    RETURN $1;\n",
    "END;\n",
    "$$ LANGUAGE plpgsql;\n",
    "You can read the cursor content by calling the function with a regular, non-named, Psycopg cursor:\n",
    "\n",
    "cur1 = conn.cursor()\n",
    "cur1.callproc('reffunc', ['curname'])\n",
    "and then use a named cursor in the same transaction to “steal the cursor”:\n",
    "\n",
    "cur2 = conn.cursor('curname')\n",
    "for record in cur2:     # or cur2.fetchone, fetchmany...\n",
    "    # do something with record\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Соответствие типов данных Python и PostgreSQL"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Python\t\tPostgreSQL\n",
    "None\t\tNULL\n",
    "bool\t\tbool\n",
    "float\t\treal\n",
    "\t\t\tdouble\n",
    "            float\n",
    "int \t\tsmallint\n",
    "\t\t\tinteger\n",
    "long\t\tbigint\n",
    "Decimal\t\tnumeric\n",
    "str\t\t\tvarchar\n",
    "unicode\t\ttext\n",
    "date\t\tdate\n",
    "time\t\ttime\n",
    "\t\t\ttimetz\n",
    "datetime\ttimestamp\n",
    "\t\t\ttimestamptz\n",
    "timedelta\tinterval\n",
    "list\t\tARRAY\n",
    "tuple \t\tComposite types\n",
    "namedtuple\tIN syntax\n",
    "dict\t\thstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sql = \"\"\"\n",
    "create or replace table mytable as -- mytable example\n",
    "seLecT a.asdf, b.qwer, -- some comment here\n",
    "c.asdf, -- some comment there\n",
    "b.asdf2 frOm table1 as a leFt join \n",
    "table2 as b -- and here a comment\n",
    "    on a.asdf = b.asdf  -- join this way\n",
    "    inner join table3 as c\n",
    "on a.asdf=c.asdf\n",
    "whEre a.asdf= 1 -- comment this\n",
    "anD b.qwer =2 and a.asdf<=1 --comment that\n",
    "or b.qwer>=5\n",
    "groUp by a.asdf\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE OR REPLACE TABLE mytable AS -- mytable example\n",
      "SELECT a.asdf,\n",
      "       b.qwer, -- some comment here\n",
      "       c.asdf, -- some comment there\n",
      "       b.asdf2\n",
      "FROM   table1 as a\n",
      "    LEFT JOIN table2 as b -- and here a comment\n",
      "        ON a.asdf = b.asdf -- join this way\n",
      "    INNER JOIN table3 as c\n",
      "        ON a.asdf = c.asdf\n",
      "WHERE  a.asdf = 1 -- comment this\n",
      "   and b.qwer = 2\n",
      "   and a.asdf <= 1 --comment that\n",
      "    or b.qwer >= 5\n",
      "GROUP BY a.asdf\n"
     ]
    }
   ],
   "source": [
    "from sql_formatter.core import format_sql\n",
    "print(format_sql(example_sql))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.debug('Start')\n",
    "logging.warning('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random \n",
    "import logging\n",
    "\n",
    "current_filename = os.path.basename(__file__).rsplit('.', 1)[0]\n",
    "\n",
    "# changing level we can change frome what level we want to log the events\n",
    "\n",
    "logging.basicConfig(filename = current_filename + '.log', level = logging.INFO, format = '%(asctime)s:%(levelname)s:%(message)s')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.info('Generate some random integers')\n",
    "\n",
    "    num_numbers = 10\n",
    "    min_value = 0\n",
    "    max_value = 20\n",
    "    warning_threshold = 5\n",
    "    error_threshold = 10\n",
    "    critical_threshold = 15\n",
    "\n",
    "    logging.debug('Numbers: {}\\nMin value: {}\\nMax value: {}\\nWaring threshold: {}\\nError threshold: {}\\nCritical threshold: {}'.format(num_numbers, min_value, max_value, warning_threshold, error_threshold, critical_threshold))\n",
    "\n",
    "    logging.debug('Start')\n",
    "    for i in range(num_numbers):\n",
    "        logging.debug('Iteration: {}'.format(i))\n",
    "        value = random.randint(min_value, max_value)\n",
    "\n",
    "        try:\n",
    "            logging.debug('\\tTry value')\n",
    "            if value > critical_threshold:\n",
    "                raise Exception('\\tValue: {} -> Critical!'.format(value))\n",
    "            elif value > error_threshold:\n",
    "                logging.error('\\tValue: {} -> Error!'.format(value))\n",
    "            elif value > warning_threshold:\n",
    "                logging.warning('\\tValue: {} -> Warning!'.format(value))\n",
    "            else:\n",
    "                logging.info('\\tValue: {}'.format(value))\n",
    "        except Exception as e:\n",
    "            logging.critical(e)\n",
    "            exit()\n",
    "        finally:\n",
    "            logging.debug('The try except is finished')\n",
    "\n",
    "    logging.debug('End')\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## psycopg2 connection v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "\n",
    "class PostgreSQL_conn():\n",
    "    def __init__(self, params_dict=None):\n",
    "        if params_dict is None:\n",
    "            self.params_dict = {\n",
    "                'user': os.environ['USERNAME'],\n",
    "                'password': os.environ['POSTGRESQL_PASSWORD'],\n",
    "                'host': '***',\n",
    "                'port': '**',\n",
    "                'database': '**'\n",
    "            }\n",
    "        else:\n",
    "            self.params_dict = params_dict\n",
    "    \n",
    "    def show_exception(self, err):\n",
    "        err_type, err_obj, traceback = sys.exc_info()    \n",
    "        line_n = traceback.tb_lineno    \n",
    "        print(\"\\npsycopg2 ERROR:\", err, \"on line number:\", line_n)\n",
    "        print(\"psycopg2 traceback:\", traceback, \"-- type:\", err_type) \n",
    "        print(err)   \n",
    "        \n",
    "    def connect(self):\n",
    "        try:\n",
    "            conn = psycopg2.connect(**self.params_dict)\n",
    "            cursor = conn.cursor()\n",
    "        except Exception as err:\n",
    "            self.show_exception(err)        \n",
    "            conn = None\n",
    "            cursor = None\n",
    "        self.conn, self.cursor = conn, cursor\n",
    "        return conn, cursor\n",
    "    \n",
    "    def to_alchemy(self, df, table_name, mode='append', db='postgresql', schema='analytics'):\n",
    "        try:\n",
    "            url = '{}+psycopg2://{}:{}@{}:{}/{}'.format(\n",
    "                db,\n",
    "                self.params_dict['user'],\n",
    "                self.params_dict['password'],\n",
    "                self.params_dict['host'],\n",
    "                self.params_dict['port'],\n",
    "                self.params_dict['database']\n",
    "            )\n",
    "            engine = create_engine(url)\n",
    "            df.to_sql(\n",
    "                table_name,\n",
    "                schema = schema,\n",
    "                con = engine, \n",
    "                index = False, \n",
    "                if_exists = mode\n",
    "            )\n",
    "        except Exception as err:\n",
    "            self.show_exception(err)\n",
    "    \n",
    "    def create_table(self, table_name, attrs_sql):\n",
    "        self.cursor.execute(f'CREATE TABLE {table_name} ({attrs_sql}) IF NOT EXISTS')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## psycopg2 connection v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import Error\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n",
    "try:\n",
    "    # Подключение к существующей базе данных\n",
    "    connection = psycopg2.connect(user=\"postgres\",\n",
    "                                  # пароль, который указали при установке PostgreSQL\n",
    "                                  password=\"1111\",\n",
    "                                  host=\"127.0.0.1\",\n",
    "                                  port=\"5432\")\n",
    "    connection.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "    # Курсор для выполнения операций с базой данных\n",
    "    cursor = connection.cursor()\n",
    "    sql_create_database = 'create database postgres_db'\n",
    "    cursor.execute(sql_create_database)\n",
    "except (Exception, Error) as error:\n",
    "    print(\"Ошибка при работе с PostgreSQL\", error)\n",
    "finally:\n",
    "    if connection:\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        print(\"Соединение с PostgreSQL закрыто\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class SparkHelper:\n",
    "    \"\"\"Класс содержит функции выгрузки и загрузки данных через спарк\"\"\"\n",
    "\n",
    "    def create_sql_context(self, config=None, name='None', port='4440', instances=5, n_cores=2, spark_version='2.4',  name_nodes=['CLSKLCIB', 'CLSKLSBX', 'SUPERCLUSTER'], executor_memory='10g', driver_memory='10g'):\n",
    "        \"\"\"\n",
    "        Создание спарк контекста\n",
    "\n",
    "        config:\n",
    "            Конфиг контекста\n",
    "        name:\n",
    "            имя процесса\n",
    "        port:\n",
    "            spark.ui.port\n",
    "        instances:\n",
    "            кол-во instance\n",
    "        n_cores:\n",
    "            кол-во cores\n",
    "        spark_version:\n",
    "            '2.1', '2.2', '3'\n",
    "\n",
    "        \"\"\"\n",
    "        if spark_version=='3':\n",
    "            print('spark_version :' +spark_version)\n",
    "            os.environ['SPARK_MAJOR_VERSION'] = '3'\n",
    "            os.environ['SPARK_HOME'] = '/usr/sdp/3.4.0.1-1/spark3/'\n",
    "            os.environ['PYSPARK_DRIVER_PYTHON'] = 'python'\n",
    "            os.environ['PYSPARK_PYTHON'] = '/data/parcels/PYENV.ZNO20008661.Spark3.0.1-3.5.pyenv.p0.1/bin/python'\n",
    "            os.environ['LD_LIBRARY_PATH'] = '/opt/python/virtualenv/jupyter/lib'\n",
    "\n",
    "            sys.path.insert(0, '/usr/sdp/3.4.0.1-1/spark3/python/')\n",
    "            sys.path.insert(0, '/usr/sdp/3.4.0.1-1/spark3/python/lib/py4j-0.10.7-src.zip')\n",
    "        else:\n",
    "            print('spark_version :' +spark_version)\n",
    "            os.environ['SPARK_MAJOR_VERSION'] = '2'\n",
    "            os.environ['SPARK_HOME'] = '/usr/sdp/3.4.0.1-1/spark2/'\n",
    "            os.environ['PYSPARK_PYTHON'] = '/data/parcels/PYENV.ZNO20008661-3.5.pyenv.p0.20/bin/python'\n",
    "            os.environ['PYSPARK_DRIVER_PYTHON'] = 'python'\n",
    "            os.environ['LD_LIBRARY_PATH'] = '/opt/python/virtualenv/jupyter/lib'\n",
    "            \n",
    "            sys.path.insert(0, '/usr/sdp/3.4.0.1-1/spark2/python/')\n",
    "            sys.path.insert(0, '/usr/sdp/3.4.0.1-1/spark2/python/lib/py4j-0.10.7-src.zip')\n",
    "\n",
    "        from pyspark import SparkContext, SparkConf, HiveContext\n",
    "\n",
    "        if config is None:\n",
    "            config = SparkConf().setMaster(\"yarn\").setAppName(name)\n",
    "\n",
    "            config.setAll(\n",
    "                [  \n",
    "                   ('spark.local.dir', 'sparktmp'),\n",
    "                   ('hive.exec.dynamic.partition.mode', 'nonstrict'),\n",
    "                   ('spark.executor.memory',executor_memory),\n",
    "                   ('spark.driver.memory',driver_memory),\n",
    "                   ('spark.driver.maxResultSize','15g'),\n",
    "                   ('spark.executor.instances', instances),\n",
    "                   ('spark.default.parallelism', '1000'),\n",
    "                   ('spark.port.maxRetries', '500'),\n",
    "                   ('spark.executor.cores', n_cores),\n",
    "                   ('spark.dynamicAllocation.enabled', 'false'),\n",
    "                   ('spark.ui.port', port),\n",
    "                   ('spark.blacklist.enabled', 'false'),\n",
    "                   ('spark.sql.shuffle.partitions', '400'),\n",
    "                   ('spark.blacklist.task.maxTaskAttemptsPerNode', '15'),\n",
    "                   ('spark.blacklist.task.maxTaskAttemptsPerExecutor', '15'),\n",
    "                   ('spark.task.maxFailures', '50'),\n",
    "                   ('spark.yarn.access.namenodes', 'hdfs://clsklcib:8020,hdfs://clsklsbx:8020,hdfs://hdfsgw:8020,hdfs://nsld3:8020,hdfs://nsld3:8020,hdfs://clsklrozn:8020,hdfs://clsklarnsdpsbx:8020',)\n",
    "                ])\n",
    "\n",
    "        print('Start')\n",
    "        self.sc = SparkContext.getOrCreate(conf=config)\n",
    "        self.sqlContext = HiveContext(self.sc)\n",
    "        print('Context ready: %s' % self.sc)\n",
    "\n",
    "        return self.sc, self.sqlContext\n",
    "\n",
    "\n",
    "    def save_to_csv(self, df, sep:str, username:str,  hdfs_path:str, local_path:str=None, isHeader='true'):\n",
    "        \"\"\"\n",
    "        Сохраняет Spark DataFrame с csv и создает линк на этот файл в файловой системе Jupyter\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        username:\n",
    "            Имя пользователя в ЛД\n",
    "        hdfs_path:\n",
    "            Путь для сохранения файла в HDFS относительно папки пользователя (например notebooks/data)\n",
    "        local_path:\n",
    "            Путь, по которому будет доступен файл в файловой системе Jupyter (/home)\n",
    "            Если None - запись производится только в hdfs\n",
    "        \"\"\"\n",
    "        import subprocess\n",
    "        import os\n",
    "\n",
    "        df.write \\\n",
    "            .format('com.databricks.spark.csv') \\\n",
    "            .mode('overwrite') \\\n",
    "            .option('sep', sep) \\\n",
    "            .option('header', isHeader) \\\n",
    "            .option(\"quote\", '\\u0000') \\\n",
    "            .save(hdfs_path)\n",
    "\n",
    "        if local_path!=None:\n",
    "            path_to_hdfs = os.path.join('/user', username, hdfs_path)\n",
    "            path_to_local = os.path.join(local_path)\n",
    "            proc = subprocess.Popen(['hdfs', 'dfs', '-getmerge', path_to_hdfs, path_to_local])\n",
    "            proc.communicate()\n",
    "\n",
    "            columns_row = sep.join(df.columns)\n",
    "            os.system(\"sed -i -e 1i'\" + columns_row + \"\\\\' \" + path_to_local)\n",
    "\n",
    "\n",
    "    def read_from_csv(self, hdfs_path:str, schema=None):\n",
    "        \"\"\"\n",
    "        Чтения из csv в Spark Dataframe\n",
    "        Parameters\n",
    "        ----------\n",
    "        hdfs_path:\n",
    "            Путь файла в HDFS относительно папки пользователя (например notebooks/data)\n",
    "        schema:\n",
    "            Схема таблицы\n",
    "        \"\"\"\n",
    "\n",
    "        if schema is None:\n",
    "            return sqlContext.read.format('com.databricks.spark.csv') \\\n",
    "            .option('inferSchema', 'true') \\\n",
    "            .option('header', 'true') \\\n",
    "            .option('delimiter', ';') \\\n",
    "            .option('decimal', '.') \\\n",
    "            .option('dateFormat', 'yyyy-MM-dd') \\\n",
    "            .option('encoding', 'cp1251') \\\n",
    "            .load(hdfs_path)\n",
    "        else:\n",
    "            return sqlContext.read.format('com.databricks.spark.csv') \\\n",
    "                .schema(schema) \\\n",
    "                .option('header', 'true') \\\n",
    "                .option('delimiter', ';') \\\n",
    "                .option('decimal', '.') \\\n",
    "                .option('dateFormat', 'yyyy-MM-dd') \\\n",
    "                .option('encoding', 'cp1251') \\\n",
    "                .load(hdfs_path)\n",
    "\n",
    "\n",
    "    def create_table_from_select(self, db_in='t_ural_kb', table_name='', db_out='t_ural_kb', prefix='0_'):\n",
    "        \"\"\"\n",
    "        Для копирования таблиц из схемы в схему\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        db_in:\n",
    "             схема из которой нужно копировать\n",
    "        table:\n",
    "             копируемая таблица\n",
    "        db_out:\n",
    "             схема в которую копируем\n",
    "        \"\"\"\n",
    "\n",
    "        start=time.time()\n",
    "        print(\"Start copying in {db_in}.{table} to {db_out}.{table}\".format(db_in=db_in, table=prefix+table_name, db_out=db_out))\n",
    "\n",
    "        self.sqlContext.sql('DROP TABLE IF EXISTS {db_out}.{tab} purge'.format(db_out=db_out, tab=prefix+table_name))\n",
    "        sql_query=\"\"\"create table {db_out}.{prefix}{table}\n",
    "                            as\n",
    "                            select *\n",
    "                            from {db_in}.{table}\"\"\".format(db_in=db_in, table=table_name, prefix=prefix, db_out=db_out)\n",
    "\n",
    "        self.sqlContext.sql(sql_query)\n",
    "\n",
    "        print(\"End copying in {db_in}.{table} to {db_out}.{table} ...  {t:0.2f} seconds \".format(db_in=db_in, table=prefix+table_name, db_out=db_out, t=(start-time.time())))\n",
    "\n",
    "    def save_to_hive(self, df, db='t_ural_kb', table_name='temp_{d:}'.format(d=datetime.now().date()).replace('-', '_')):\n",
    "        \"\"\"\n",
    "        Сохранение spark датафрейма в hive\n",
    "        Parameters\n",
    "        ----------\n",
    "        df:\n",
    "            spark датафрейм для записи в таблицу\n",
    "        db:\n",
    "            схема в которой создается таблица\n",
    "        table_name:\n",
    "            название таблицы, если не указано temp_{текущая дата} (temp_01_07_2019)\n",
    "        \"\"\"\n",
    "        start=time.time()\n",
    "        df.registerTempTable(table_name)\n",
    "        self.sqlContext.sql('DROP TABLE IF EXISTS {db}.{tab} purge'.format(db=db, tab=table_name))\n",
    "        self.sqlContext.sql('CREATE TABLE {db}.{tab} SELECT * FROM {tab}'.format(db=db, tab=table_name))\n",
    "        print(\"End creating {db}.{table}...  {t:0.2f} seconds \".format(db=db, table=table_name, t=(time.time()-start)))\n",
    "\n",
    "\n",
    "    def get_table (self, db, table_name, columns=[]):\n",
    "        \"\"\"\n",
    "        Селект таблицы\n",
    "        Parameters\n",
    "        ----------\n",
    "        db:\n",
    "            схема из которой селектится таблица\n",
    "        table_name:\n",
    "            название таблицы\n",
    "        columns:\n",
    "            список колонок для селекта, если пустой *\n",
    "        \"\"\"\n",
    "        return self.sqlContext.sql('SELECT {cols} FROM {db}.{tab}'.format(db=db, tab=table_name, cols=', '.join(columns) if columns!=[] else '*'))\n",
    "\n",
    "    def add_prefix_col_name(self, df, prefix):\n",
    "        \"\"\"\n",
    "        Добавление префикса ко всем названиям колонок в таблице\n",
    "        Parameters\n",
    "        ----------\n",
    "        df:\n",
    "            датафрейм в котором нужно переименовать столбцы\n",
    "        prefix:\n",
    "            префикс для столбцов\n",
    "\n",
    "        \"\"\"\n",
    "        for col in df.columns:\n",
    "            df=df.withColumnRenamed(col, prefix+'_'+col)\n",
    "        return df\n",
    "\n",
    "    def cast_columns(self, df, columns_types):\n",
    "        \"\"\"\n",
    "        Приведение типов к списку колонок\n",
    "        Parameters\n",
    "        ----------\n",
    "        df:\n",
    "            спарк датафрейм\n",
    "        columns_types:\n",
    "            словарь соответствий колонка : тип к которому нужно привести колонку\n",
    "        \"\"\"\n",
    "        from pyspark.sql import functions  as F\n",
    "        for col_name, type_col in columns_types.items():\n",
    "            df=df.withColumn(col_name, F.col(col_name).cast(type_col))\n",
    "\n",
    "        return df\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
